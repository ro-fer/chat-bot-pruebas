{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f766c8d",
   "metadata": {},
   "source": [
    "# Laboratorio Módulo 3: Deployment y Consumo de LLaMa 3.1 con Ollama\n",
    "\n",
    "## Llama 3.1 - Capacitación SICYT Argentina 2025\n",
    "\n",
    "**Capacitación:** SICYT Argentina 2025  \n",
    "**Fecha:** Octubre 2025\n",
    "\n",
    "**Objetivo:**  \n",
    "Aprender a instalar y usar Ollama para ejecutar modelos LLaMa en CPU y consumir el modelo desde Python en Visual Studio Code usando la API REST. Incluye instalación de herramientas y ejercicios prácticos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc26fc53",
   "metadata": {},
   "source": [
    "## 0. Instalación de Herramientas\n",
    "\n",
    "### 0.1 Instalar Visual Studio Code\n",
    "\n",
    "- Descargar Visual Studio Code desde: [https://code.visualstudio.com/](https://code.visualstudio.com/)\n",
    "- Instalar como cualquier programa en Windows o Mac.\n",
    "\n",
    "### 0.2 Instalar Python 3.10 o superior\n",
    "\n",
    "- Descargar Python desde: [https://www.python.org/downloads/](https://www.python.org/downloads/)\n",
    "- Durante la instalación en Windows, marcar la opción **\"Add Python to PATH\"**.\n",
    "- En Mac, puedes usar el instalador o `brew install python`.\n",
    "\n",
    "### 0.3 Instalar extensiones en VS Code\n",
    "\n",
    "1. Abre VS Code.\n",
    "2. Ve a la barra lateral izquierda (ícono de cuadrados) o presiona `Ctrl+Shift+X`.\n",
    "3. Busca e instala:\n",
    "    - **Python** (de Microsoft)\n",
    "    - **Jupyter** (de Microsoft)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fc89b7",
   "metadata": {},
   "source": [
    "## 1. Preparación del entorno de trabajo\n",
    "\n",
    "### 1.1 Crear y activar un entorno virtual (opcional pero recomendado)\n",
    "\n",
    "Abre la terminal integrada en VS Code (`Ctrl+ñ` o desde menú Terminal > Nueva terminal).\n",
    "\n",
    "#### **Windows:**\n",
    "```bash\n",
    "python -m venv venv\n",
    ".\\venv\\Scripts\\activate\n",
    "```\n",
    "\n",
    "#### **Mac:**\n",
    "```bash\n",
    "python3 -m venv venv\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "### 1.2 Instalar dependencias Python\n",
    "```bash\n",
    "pip install jupyter requests\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e13206",
   "metadata": {},
   "source": [
    "## 2. Instalación y prueba de Ollama\n",
    "\n",
    "### 2.1 Instalar Ollama\n",
    "\n",
    "- **Windows:**  \n",
    "  - Descarga el instalador desde [https://ollama.ai/download](https://ollama.ai/download) y sigue las instrucciones. Puede requerir WSL2 (se instala automáticamente si es necesario).\n",
    "- **Mac:**  \n",
    "  - Puedes instalar desde terminal con:\n",
    "    ```bash\n",
    "    brew install ollama\n",
    "    ```\n",
    "    O descarga el instalador desde [https://ollama.ai/download](https://ollama.ai/download).\n",
    "\n",
    "### 2.2 Verificar la instalación\n",
    "Abre una terminal (puede ser la misma de VS Code) y ejecuta:\n",
    "```bash\n",
    "ollama --version\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ac1860",
   "metadata": {},
   "source": [
    "## 3. Descargar y ejecutar LLaMa 3.1 en Ollama\n",
    "\n",
    "### 3.1 Descargar modelo\n",
    "```bash\n",
    "ollama pull llama3\n",
    "```\n",
    "\n",
    "### 3.2 Probar el modelo desde la terminal\n",
    "```bash\n",
    "ollama run llama3 \"¿Qué es la inteligencia artificial?\"\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9cac56",
   "metadata": {},
   "source": [
    "## 4. Consumir Ollama desde Python usando la API REST\n",
    "> Antes de seguir, asegúrate que Ollama está corriendo (el instalador inicia el servicio automáticamente; si no, ejecuta `ollama serve`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d285d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "OLLAMA_HOST = \"http://127.0.0.1:11434\"\n",
    "\n",
    "prompt = \"Explica brevemente el concepto de aprendizaje profundo.\"\n",
    "payload = {\n",
    "    \"model\": \"llama3.1\",\n",
    "    \"prompt\": prompt,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(f\"{OLLAMA_HOST}/api/generate\", json=payload)\n",
    "result = response.json()\n",
    "print(\"Respuesta del modelo:\")\n",
    "print(result.get(\"response\", \"Error\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65d18a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4.2 Experimenta con diferentes parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcdc0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for temperature in [0.3, 0.7, 1.0]:\n",
    "    payload = {\n",
    "        \"model\": \"llama3.1\",\n",
    "        \"prompt\": \"Resume la importancia de los transformers en el procesamiento de lenguaje natural.\",\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": 0.9,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(f\"{OLLAMA_HOST}/api/generate\", json=payload)\n",
    "    print(f\"\\nTemperature={temperature}:\")\n",
    "    print(response.json().get(\"response\", \"Error\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2306e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4.3 Probar streaming de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574e3fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"llama3\",\n",
    "    \"prompt\": \"Nombra tres aplicaciones prácticas de los LLMs en empresas.\",\n",
    "    \"stream\": True\n",
    "}\n",
    "response = requests.post(f\"{OLLAMA_HOST}/api/generate\", json=payload, stream=True)\n",
    "print(\"Respuesta en streaming:\")\n",
    "for line in response.iter_lines():\n",
    "    if line:\n",
    "        chunk = json.loads(line)\n",
    "        print(chunk.get(\"response\", \"\"), end=\"\", flush=True)\n",
    "        if chunk.get(\"done\"):\n",
    "            print(\"\\n--- Generación completada ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82ab04b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Manejo básico de errores en la API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33ff5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error inesperado: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=40)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    payload = {\n",
    "        \"model\": \"llama3\",\n",
    "        \"prompt\": \"¿Cuál es el futuro de la IA?\",\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(f\"{OLLAMA_HOST}/api/generate\", json=payload, timeout=40)\n",
    "    response.raise_for_status()\n",
    "    print(response.json().get(\"response\", \"\"))\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Error: Ollama no está corriendo en localhost:11434\")\n",
    "except Exception as e:\n",
    "    print(\"Error inesperado:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7675d6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. (Opcional) Mini servidor Flask para consumir Ollama desde web\n",
    "\n",
    "**Ejecuta este script por separado si tienes Flask instalado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d7ed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    }
   ],
   "source": [
    "# Guardar como app.py y ejecutar con: python app.py\n",
    "from flask import Flask, request, jsonify\n",
    "import requests\n",
    "\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/genera', methods=['POST'])\n",
    "def genera():\n",
    "    datos = request.json\n",
    "    prompt = datos.get(\"prompt\", \"\")\n",
    "    payload = {\n",
    "        \"model\": \"llama3.1\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    resp = requests.post(f\"{OLLAMA_HOST}/api/generate\", json=payload)\n",
    "    return jsonify(resp.json())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=5000, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00991b62",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Preguntas y ejercicios de reflexión\n",
    "\n",
    "1. **¿Qué diferencias observas en las respuestas del modelo cuando cambias el parámetro `temperature`?**\n",
    "2. **¿Cómo podrías integrar Ollama en una aplicación real de atención al cliente?**\n",
    "3. **¿Qué limitaciones encontraste usando la API REST de Ollama?**\n",
    "4. **Prueba agregar un endpoint extra en Flask que realice streaming de la respuesta (requiere manejo avanzado de streams en Flask).**\n",
    "\n",
    "---\n",
    "\n",
    "**¡Fin del laboratorio!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
